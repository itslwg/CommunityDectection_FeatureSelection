{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run hyper-parameter tuning for data preparation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ludvigwgerdin/courses/Financial Big Data/FinanicalBigData-EPFL/code'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.feather as feather\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from utils.cv import PurgedGroupTimeSeriesSplit\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from utils.metrics import utility_score\n",
    "from utils.data_preparation import preprocess\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2390491 entries, 0 to 2390490\n",
      "Columns: 138 entries, date to ts_id\n",
      "dtypes: float64(135), int64(3)\n",
      "memory usage: 2.5 GB\n"
     ]
    }
   ],
   "source": [
    "df = feather.read_feather(\"../data/raw/train.feather\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the memory usage by half when importing the data. \n",
    "Source: https://www.kaggle.com/jorijnsmit/one-liner-to-halve-your-memory-usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2390491 entries, 0 to 2390490\n",
      "Columns: 138 entries, date to ts_id\n",
      "dtypes: float32(135), int64(3)\n",
      "memory usage: 1.3 GB\n"
     ]
    }
   ],
   "source": [
    "float64_cols = df.select_dtypes(include='float64').columns\n",
    "mapper = {col_name: np.float32 for col_name in float64_cols}\n",
    "df = df.astype(mapper, copy=False)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label\n",
    "df[\"action\"] = np.where(df[\"resp\"]*df[\"weight\"]>0, 1, 0)\n",
    "# Get feature names\n",
    "features = df.columns[df.columns.str.contains(\"feature\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train and test sets. \n",
    "Test set is used to compare the different methods.  There are 500 days in total.  We use 400 days to train and 90 days to test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap = 10\n",
    "split = 400\n",
    "train = df[df.date <= split]\n",
    "test = df[df.date > split+gap]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Gap group TS CV.\n",
    "K = 5\n",
    "tscv = PurgedGroupTimeSeriesSplit(n_splits=K, \n",
    "                                  max_train_group_size=189,\n",
    "                                  group_gap=gap, \n",
    "                                  max_test_group_size=63\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for feature selection method.  \n",
    "param_grid_selection = {\n",
    "    'impute_strategy': ['mean', 'median'],\n",
    "    'K': ['n_clusters', 10, 20, 30, 40]\n",
    "}\n",
    "combinations_selection = generate_all_combinations(param_grid_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for merging features method.\n",
    "param_grid_subcluster = {\n",
    "    'impute_strategy': ['mean', 'median'],\n",
    "    'agg': [np.mean, np.prod, np.sum]\n",
    "}\n",
    "combinations_subcluster = generate_all_combinations(param_grid_subcluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Combinations for intra cluster feature selection 8 \n",
      "# Combination for merging subclusters 6\n"
     ]
    }
   ],
   "source": [
    "print(\"# Combinations for intra cluster feature selection\",len(combinations_selection),\n",
    "      \"\\n# Combination for merging subclusters\",len(combinations_subcluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_evaluate(df, X_tr_sc, X_te_sc, \n",
    "                         X_te, y_tr, \n",
    "                         tr, te,\n",
    "                         lgbm_all_f):\n",
    "    # Train LGBM with new features\n",
    "    lgbm_sc = LGBMClassifier().fit(X_tr_sc, y_tr)\n",
    "    \n",
    "    # Compute predictions for both LGBM.\n",
    "    y_pred_all_f = lgbm_all_f.predict(X_te)\n",
    "    y_pred_sc = lgbm_sc.predict(X_te_sc)\n",
    "\n",
    "    # Compute scoring metrics.\n",
    "    score_te_all_f = utility_score(\n",
    "        date=df[\"date\"].iloc[te].values,\n",
    "        weight=df[\"weight\"].iloc[te].values,\n",
    "        resp=df[\"resp\"].iloc[te].values,\n",
    "        action=y_pred_all_f\n",
    "    )\n",
    "    \n",
    "    score_te_sc = utility_score(\n",
    "        date=df[\"date\"].iloc[te].values,\n",
    "        weight=df[\"weight\"].iloc[te].values,\n",
    "        resp=df[\"resp\"].iloc[te].values,\n",
    "        action=y_pred_sc\n",
    "    )\n",
    "    \n",
    "    return score_te_all_f, score_te_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_comb_performance(train, comb, method=1):\n",
    "    \"\"\"\n",
    "    method=1 is cluster weighted feature selection.\n",
    "    method=2 is subcluster feature merging\n",
    "    \"\"\"\n",
    "    utility = []\n",
    "    for fold, (tr, te) in enumerate(tscv.split(X=train, y=train[\"action\"], groups=train[\"date\"])):    \n",
    "        print(f\"Fold: {fold}\")\n",
    "        \n",
    "        # Split train and test for X and y in CV.\n",
    "        X_tr, y_tr = train[features].iloc[tr], train[\"action\"].iloc[tr]\n",
    "        X_te, y_te = train[features].iloc[te], train[\"action\"].iloc[te]\n",
    "    \n",
    "        # Standardize, community detection, and feature selection\n",
    "        X_tr_sc, X_te_sc, lgbm_all_f = preprocess(\n",
    "            df = train,\n",
    "            X_tr=X_tr,\n",
    "            y_tr=y_tr,\n",
    "            X_te=X_te,\n",
    "            tr=tr,\n",
    "            comb=comb,\n",
    "            method=method\n",
    "        )\n",
    "        \n",
    "        # Model fitting and evaluation\n",
    "        score_te_all_f, score_te_sc = predict_and_evaluate(\n",
    "            train, X_tr_sc, X_te_sc, X_te, y_tr, tr, te, lgbm_all_f\n",
    "        )\n",
    "        \n",
    "        utility.append({\"all_features\":score_te_all_f,\n",
    "                        \"reduced_features\":score_te_sc})\n",
    "    utility_df = pd.DataFrame(utility)\n",
    "    return utility_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOBLIB_TEMP_FOLDER=/tmp\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "%env JOBLIB_TEMP_FOLDER=/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_selection = Parallel(n_jobs=2)(delayed(compute_comb_performance)(train, comb, method=1) for comb in combinations_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_selection = pd.concat(results_selection, axis=1)\n",
    "best_parameters_selection = combinations_selection[np.argmax(scores_selection.loc[\"reduced_features\", :])]\n",
    "best_parameters_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import pickle\n",
    "#pickle.dump(results_selection, open(\"../data/clean/results_selection.pickle\", \"wb\"))\n",
    "#pickle.dump(best_parameters_selection, open(\"../data/clean/best_parameters_selection.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the selection method, the gridsearch was accidentally run without one of the options. The best params are determined by merging results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_selection_n_clusters = pickle.load(open(\"../data/clean/results_selection_n_clusters.pickle\", \"rb\"))\n",
    "results_selection_wo_n_clusters = pickle.load(open(\"../data/clean/results_selection_wo_n_clusters.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_selection = pd.concat(results_selection_n_clusters + results_selection_wo_n_clusters, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>all_features</th>\n",
       "      <td>191.738808</td>\n",
       "      <td>205.998504</td>\n",
       "      <td>191.738808</td>\n",
       "      <td>191.738808</td>\n",
       "      <td>191.738808</td>\n",
       "      <td>191.738808</td>\n",
       "      <td>205.998504</td>\n",
       "      <td>205.998504</td>\n",
       "      <td>205.998504</td>\n",
       "      <td>205.998504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reduced_features</th>\n",
       "      <td>39.292231</td>\n",
       "      <td>32.728847</td>\n",
       "      <td>41.968317</td>\n",
       "      <td>194.776724</td>\n",
       "      <td>131.787935</td>\n",
       "      <td>150.144350</td>\n",
       "      <td>109.495302</td>\n",
       "      <td>181.436749</td>\n",
       "      <td>179.904207</td>\n",
       "      <td>185.570160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0           1           2           3           4  \\\n",
       "all_features      191.738808  205.998504  191.738808  191.738808  191.738808   \n",
       "reduced_features   39.292231   32.728847   41.968317  194.776724  131.787935   \n",
       "\n",
       "                           5           6           7           8           9  \n",
       "all_features      191.738808  205.998504  205.998504  205.998504  205.998504  \n",
       "reduced_features  150.144350  109.495302  181.436749  179.904207  185.570160  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two columns correspond to the gridsearch with only the n_clusters option for K. We see that an approach from the gridsearch without the option \"n_clusters\" performed best, so we use that in the results notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'impute_strategy': 'mean',\n",
       " 'agg': <function numpy.sum(a, axis=None, dtype=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)>}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_subcluster = pd.concat(results_subcluster, axis=1)\n",
    "best_parameters_subcluster = combinations_subcluster[np.argmax(scores_subcluster.loc[\"reduced_features\", :])]\n",
    "best_parameters_subcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>all_features</th>\n",
       "      <td>188.069953</td>\n",
       "      <td>188.069953</td>\n",
       "      <td>188.069953</td>\n",
       "      <td>184.005060</td>\n",
       "      <td>184.005060</td>\n",
       "      <td>184.005060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reduced_features</th>\n",
       "      <td>177.831777</td>\n",
       "      <td>99.927195</td>\n",
       "      <td>188.796126</td>\n",
       "      <td>170.065785</td>\n",
       "      <td>84.249073</td>\n",
       "      <td>150.080256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0           1           2           3           4  \\\n",
       "all_features      188.069953  188.069953  188.069953  184.005060  184.005060   \n",
       "reduced_features  177.831777   99.927195  188.796126  170.065785   84.249073   \n",
       "\n",
       "                           5  \n",
       "all_features      184.005060  \n",
       "reduced_features  150.080256  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_subcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "pickle.dump(best_parameters_subcluster, open(\"../data/clean/best_parameters_subclusters.pickle\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
